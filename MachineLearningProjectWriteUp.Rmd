---
title: "R Notebook"
output: html_notebook
> library(caret)
Loading required package: lattice
Loading required package: ggplot2
Warning message:
package 'caret' was built under R version 3.3.3 
> library(rpart)
> library(rpart.plot)
Warning message:
package 'rpart.plot' was built under R version 3.3.3 
> library(rattle)
Rattle: A free graphical interface for data mining with R.
Version 4.1.0 Copyright (c) 2006-2015 Togaware Pty Ltd.
Type 'rattle()' to shake, rattle, and roll your data.
Warning message:
package 'rattle' was built under R version 3.3.3 
> library(randomForest)
randomForest 4.6-12
Type rfNews() to see new features/changes/bug fixes.

Attaching package: 'randomForest'

The following object is masked from 'package:ggplot2':

    margin

> library(RColorBrewer)
> trainset <- read.csv("pml-training.csv", na.strings=c("NA",""))
> testset <- read.csv("pml-testing.csv", na.strings = c("NA",""))
> trainset <- trainset[, colSums(is.na(trainset))==0]
> testset <- testset[, colSums(is.na(testset))==0]
> train_set <- trainset[, -c(1:7)]
> test_set <- test[, -c(1:7)]
> dim(train_set) ; dim(test_set)
[1] 19622    53
[1] 20 39
> set.seed(7826)
> inTrain <- createDataPartition(train_set$classe, p=0.7, list=FALSE)
> train <- train_set[inTrain, ]
> valid <- trains_et[-inTrain, ]
Error: object 'trains_et' not found
> valid <- train_set[-inTrain, ]
> dim(train); dim(test); dim(valid)
[1] 13737    53
[1] 20 46
[1] 5885   53
> control <- trainControl(method="cv", number=5)
> fit_rpart <- train(classe ~ ., data=train, method="rpart", trControl= control)
> print(fit_rpart, digits=4)
CART 

13737 samples
   52 predictor
    5 classes: 'A', 'B', 'C', 'D', 'E' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 10989, 10989, 10990, 10989, 10991 
Resampling results across tuning parameters:

  cp       Accuracy  Kappa  
  0.03723  0.5241    0.38748
  0.05954  0.4144    0.20668
  0.11423  0.3482    0.09762

Accuracy was used to select the optimal model using  the largest value.
The final value used for the model was cp = 0.03723. 
> fancyRpartPlot(fit_rpart$finalModel)
> #use the valid set
> predict_rpart <- predict(fit_rpart, valid)
> #predictyionresult
> (conf_rpart <- confusionMatrix(valid$classe, predict_rpart))
Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 1544   21  107    0    2
         B  492  391  256    0    0
         C  474   38  514    0    0
         D  436  175  353    0    0
         E  155  138  293    0  496

Overall Statistics
                                          
               Accuracy : 0.5004          
                 95% CI : (0.4876, 0.5133)
    No Information Rate : 0.5269          
    P-Value [Acc > NIR] : 1               
                                          
                  Kappa : 0.3464          
 Mcnemar's Test P-Value : NA              

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.4979  0.51245  0.33749       NA  0.99598
Specificity            0.9533  0.85396  0.88262   0.8362  0.89122
Pos Pred Value         0.9223  0.34328  0.50097       NA  0.45841
Neg Pred Value         0.6303  0.92162  0.79234       NA  0.99958
Prevalence             0.5269  0.12965  0.25879   0.0000  0.08462
Detection Rate         0.2624  0.06644  0.08734   0.0000  0.08428
Detection Prevalence   0.2845  0.19354  0.17434   0.1638  0.18386
Balanced Accuracy      0.7256  0.68321  0.61006       NA  0.94360
> # Thus Accuracy only 0.497, only 50% and the error rate would be ~50% Not so good model
> #Accuracy
> (accuracy_rpart <- conf_rpart$overall[1])
 Accuracy 
0.5004248 
> #Now applying Random Forest Algorithm
> fit_rf <- train(classe ~ ., data=train, method = "rf", trControl = control)
> #Now applying Random Forest Algorithm
> fit_rf <- train(classe ~ ., data=train, method = "rf", trControl = control)
> print(fit_rf, digits=4)
Random Forest 

13737 samples
   52 predictor
    5 classes: 'A', 'B', 'C', 'D', 'E' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 10989, 10990, 10990, 10989, 10990 
Resampling results across tuning parameters:

  mtry  Accuracy  Kappa 
   2    0.9906    0.9881
  27    0.9914    0.9891
  52    0.9856    0.9818

Accuracy was used to select the optimal model using  the largest value.
The final value used for the model was mtry = 27. 
> #apply on the validation set
> predict_rf <- predict(fit_rf, valid)
> #showing results
> (conf_rf <- confusionMatrix(valid$classe, predict_rf))
Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 1669    3    0    0    2
         B    8 1129    2    0    0
         C    0    3 1016    7    0
         D    0    1   16  944    3
         E    0    2    1    4 1075

Overall Statistics
                                          
               Accuracy : 0.9912          
                 95% CI : (0.9884, 0.9934)
    No Information Rate : 0.285           
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.9888          
 Mcnemar's Test P-Value : NA              

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.9952   0.9921   0.9816   0.9885   0.9954
Specificity            0.9988   0.9979   0.9979   0.9959   0.9985
Pos Pred Value         0.9970   0.9912   0.9903   0.9793   0.9935
Neg Pred Value         0.9981   0.9981   0.9961   0.9978   0.9990
Prevalence             0.2850   0.1934   0.1759   0.1623   0.1835
Detection Rate         0.2836   0.1918   0.1726   0.1604   0.1827
Detection Prevalence   0.2845   0.1935   0.1743   0.1638   0.1839
Balanced Accuracy      0.9970   0.9950   0.9898   0.9922   0.9970
> (accuracy_rf <- conf_rf$overall[1])
Accuracy 
0.991164 
> # Here we see the accuracy is .995 making error rate to be 0.004. Definitely RF works better then K classification
> #Finally apply on the test set
> (predict(fit_rf, testset))
 [1] B A B A A E D B A A B C B A E E A B B B
Levels: A B C D E---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

```{r}
plot(cars)
```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).
